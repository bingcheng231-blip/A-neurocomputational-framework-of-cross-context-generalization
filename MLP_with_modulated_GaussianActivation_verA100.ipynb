{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc392752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "import numpy as np  \n",
    "from torch.utils.data import DataLoader, TensorDataset  \n",
    "\n",
    "import os\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f745e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian_af(nn.Module):  \n",
    "    def __init__(self, weight_std, neuron_modulation_type, phaseR, amplitudeR):  \n",
    "        # neuron_modulation_type: none; phase; amplitude; both\n",
    "        super(Gaussian_af, self).__init__() \n",
    "        self.orignal_phase = torch.from_numpy(np.radians(np.arange(15, 360, step=30)))\n",
    "\n",
    "        neuron_sigma = torch.zeros(12, 2, 2)\n",
    "        neuron_sigma[:, 0, 0] = 0.202\n",
    "        neuron_sigma[:, 1, 1] = 0.202\n",
    "        self.sigma = neuron_sigma\n",
    "        self.pi = np.pi\n",
    "        self.eps = torch.tensor(1e-6)\n",
    "        self.phaseR = torch.tensor(phaseR)\n",
    "        self.amplitudeR = torch.tensor(amplitudeR)\n",
    "\n",
    "        if neuron_modulation_type == 'none':\n",
    "            self.phase_parameter = torch.zeros(12, 2).to(device)\n",
    "            self.amplitude_parameter = torch.zeros(12, 2).to(device)\n",
    "            self.amplitudeR = torch.tensor(0)\n",
    "\n",
    "        elif neuron_modulation_type == 'phase':\n",
    "            self.phase_parameter = nn.Parameter(torch.normal(mean=0, std=weight_std, size=(12, 2), requires_grad=True))\n",
    "            self.amplitude_parameter = torch.zeros(12, 2).to(device)\n",
    "            self.amplitudeR = torch.tensor(0)\n",
    "            \n",
    "        elif neuron_modulation_type == 'amplitude':\n",
    "            self.phase_parameter = torch.zeros(12, 2).to(device)\n",
    "            self.amplitude_parameter = nn.Parameter(torch.normal(mean=0, std=weight_std, size=(12, 2), requires_grad=True))\n",
    "        \n",
    "        elif neuron_modulation_type == 'both':\n",
    "            self.phase_parameter = nn.Parameter(torch.normal(mean=0, std=weight_std, size=(12, 2), requires_grad=True))\n",
    "            self.amplitude_parameter = nn.Parameter(torch.normal(mean=0, std=weight_std, size=(12, 2), requires_grad=True))\n",
    "\n",
    "    def forward(self, input, ctx_input):\n",
    "        orignal_phase = self.orignal_phase.to(device=device, dtype=torch.get_default_dtype())\n",
    "        phaseR = (1/self.phaseR).to(device=device, dtype=torch.get_default_dtype())\n",
    "        amplitudeR = self.amplitudeR.to(device=device, dtype=torch.get_default_dtype())\n",
    "        mvn = torch.empty(input.shape[0], 12).to(device=device, dtype=torch.get_default_dtype())\n",
    "        sigma = self.sigma.to(device=device, dtype=torch.get_default_dtype())\n",
    "\n",
    "        for ctx_i in range(2):\n",
    "            ctx_index = ctx_input[:, 0] == ctx_i\n",
    "            ctx_resp = input[ctx_index, :]\n",
    "            para_ctx = torch.tensor([[0+ctx_i], [1-ctx_i]]).to(device=device, dtype=torch.get_default_dtype())\n",
    "            # phase modulation\n",
    "            delta_phase = phaseR*2*(torch.sigmoid(self.phase_parameter@para_ctx)-0.5)*self.pi # phaseR * Â± pi\n",
    "            phase = orignal_phase.unsqueeze(1) + delta_phase\n",
    "            a = torch.cos(phase)\n",
    "            b = torch.sin(phase)\n",
    "            mu = torch.stack((a,b),axis=1).squeeze()\n",
    "            \n",
    "            # amplitude modulation\n",
    "            resp_amplitude = self.amplitude_parameter@para_ctx\n",
    "            aa = 0.202 + torch.abs(a*resp_amplitude)\n",
    "            bb = 0.202 + torch.abs(b*resp_amplitude)\n",
    "            sigma[:, 0, 0] = aa.squeeze()\n",
    "            sigma[:, 1, 1] = bb.squeeze()\n",
    "\n",
    "            batch_size = ctx_resp.shape[0]\n",
    "            # num_components = mu.shape[0]  # Assuming 12 components\n",
    "            # Ensure all tensors are on the same device and dtype\n",
    "            # input = input.to(device=device, dtype=torch.get_default_dtype())\n",
    "            # mu = mu.to(device=device, dtype=torch.get_default_dtype())\n",
    "            # sigma = sigma.to(device=device, dtype=torch.get_default_dtype())\n",
    "\n",
    "            # Expand input and mu for broadcasting\n",
    "            # input: (batch_size, 1, features)\n",
    "            # mu: (1, num_components, features)\n",
    "            input_expanded = ctx_resp.unsqueeze(1)  # (batch_size, 1, features)\n",
    "            mu_expanded = mu.unsqueeze(0)        # (1, num_components, features)\n",
    "           \n",
    "            # Compute differences for all batch samples and components at once\n",
    "            diff = input_expanded - mu_expanded  # (batch_size, num_components, features)\n",
    "            d = diff.shape[-1]\n",
    "            # Compute Cholesky decomposition for all components\n",
    "            L = torch.cholesky(sigma, upper=False)  # (num_components, features, features)\n",
    "            # Expand L for batch operations\n",
    "            L_expanded = L.unsqueeze(0).expand(batch_size, -1, -1, -1)  # (batch_size, num_components, features, features)\n",
    "            # Prepare diff for cholesky_solve\n",
    "            diff_unsqueezed = diff.unsqueeze(-1)  # (batch_size, num_components, features, 1)\n",
    "            # Solve the linear system for all samples and components\n",
    "            sol = torch.cholesky_solve(diff_unsqueezed, L_expanded, upper=False)  # (batch_size, num_components, features, 1)\n",
    "            # Compute Mahalanobis distance\n",
    "            maha = torch.sum(diff_unsqueezed.squeeze(-1) * sol.squeeze(-1), dim=-1)  # (batch_size, num_components)\n",
    "            # Compute log determinant\n",
    "            log_det = 2.0 * torch.sum(torch.log(torch.diagonal(L, dim1=-2, dim2=-1)), dim=-1)  # (num_components,)\n",
    "            log_det_expanded = log_det.unsqueeze(0).expand(batch_size, -1)  # (batch_size, num_components)\n",
    "            # Compute normalization constant\n",
    "            log_2pi = torch.log(torch.tensor(2.0 * torch.pi, dtype=diff.dtype, device=device))\n",
    "            log_norm = -0.5 * (d * log_2pi + log_det_expanded)  # (batch_size, num_components)\n",
    "            # Compute log probability density\n",
    "            log_pdf = log_norm - 0.5 * maha  # (batch_size, num_components)\n",
    "            # Convert to probability and apply scaling factor\n",
    "            ctx_mvn = torch.exp(log_pdf) / 0.7879  # (batch_size, num_components)\n",
    "            mvn[ctx_index, :] = ctx_mvn\n",
    "\n",
    "        amplitude_modulation = torch.sigmoid(self.amplitude_parameter)\n",
    "        phase_modulation = phaseR*2*(torch.sigmoid(self.phase_parameter)-0.5)*self.pi\n",
    "\n",
    "        return mvn, amplitude_modulation, phase_modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f217c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root, data_label):\n",
    "        self.data = data_root\n",
    "        self.label = data_label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        labels = self.label[index]\n",
    "        return data, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e89d6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network  \n",
    "class SimpleNN(nn.Module):  \n",
    "    def __init__(self, hiddenN, weight_std, neuron_modulation_type, phaseR, amplitudeR):  \n",
    "        super(SimpleNN, self).__init__()  \n",
    "        self.Gaussian_neuron = Gaussian_af(weight_std, neuron_modulation_type, phaseR, amplitudeR)\n",
    "        self.fc1 = nn.Linear(14, hiddenN)\n",
    "        self.bn = nn.BatchNorm1d(14)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.fc2 = nn.Linear(hiddenN, 1)  \n",
    "        # self.sigmoid = nn.Sigmoid() \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # Initialize parameters with Gaussian distribution  \n",
    "        self._initialize_weights(weight_std)  \n",
    "\n",
    "    def _initialize_weights(self, weight_std):  \n",
    "        for m in self.modules():  \n",
    "            if isinstance(m, nn.Linear):  \n",
    "                nn.init.normal_(m.weight, mean=0.0, std=weight_std)  # Gaussian initialization  \n",
    "                if m.bias is not None:  \n",
    "                    nn.init.constant_(m.bias, 0)  # Initialize bias to zero  \n",
    "\n",
    "    def forward(self, input):\n",
    "        vtc_input = input[:, :2]\n",
    "        ctx_input = input[:, 2:4]\n",
    "        vtc_input, amplitude, phase =  self.Gaussian_neuron(vtc_input, ctx_input)\n",
    "        x = torch.cat([vtc_input, ctx_input], dim=1)\n",
    "\n",
    "        x = self.fc1(x) \n",
    "        hidden_layer = x\n",
    "        x = self.relu(x)  \n",
    "        hidden_layer_relu = x\n",
    "        x = self.fc2(x) \n",
    "        output_layer = x\n",
    "        # x = self.sigmoid(x)  \n",
    "        # x = self.softmax(x)\n",
    "        return x, vtc_input, hidden_layer, hidden_layer_relu, output_layer, amplitude, phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe7cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for single-sample SGD\n",
    "def train_model_single_sample(model, criterion, optimizer, scheduler, train_datas, X_test, model_save_path, model_name):  \n",
    "    sub_train_loss = [] \n",
    "    amplitude_training = []\n",
    "    phase_training = []\n",
    "\n",
    "    prediction_mat = []\n",
    "    hidden_layer_mat=[]\n",
    "    hidden_layer_relu_mat=[]\n",
    "    output_layer_mat = []\n",
    "    amplitude_mat = []\n",
    "    phase_mat = []\n",
    "    vtc_input_mat = []\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train() \n",
    "    for i, data in enumerate(train_datas): \n",
    "        optimizer.zero_grad()  \n",
    "        output, _, _, _, _, amplitude, phase = model(data[0].to(device))  # Use a single sample  \n",
    "        \n",
    "        loss = criterion(output, data[1].to(device))  # Corresponding target  \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        sub_train_loss.append(loss.item())\n",
    "        amplitude_training.append(amplitude.cpu().detach().numpy())\n",
    "        phase_training.append(phase.cpu().detach().numpy())\n",
    "      # scheduler.step()\n",
    "\n",
    "        if (i+1)%100 == 0:\n",
    "            # Model test    \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                prediction, vtc_input, hidden_layer, hidden_layer_relu, output_layer, amplitude, phase= model(X_test)  # Use a single sample  \n",
    "                prediction_mat.append(prediction.cpu().numpy())\n",
    "                hidden_layer_mat.append(hidden_layer.cpu().numpy())\n",
    "                hidden_layer_relu_mat.append(hidden_layer_relu.cpu().numpy())\n",
    "                output_layer_mat.append(output_layer.cpu().numpy())\n",
    "                amplitude_mat.append(amplitude.cpu().numpy())\n",
    "                phase_mat.append(phase.cpu().numpy())\n",
    "                vtc_input_mat.append(vtc_input.cpu().numpy())\n",
    "\n",
    "                # mlp_weight = []\n",
    "                # state_dict = model.state_dict()\n",
    "                # for key, value in state_dict.items():\n",
    "                #     if key[:2]=='fc':\n",
    "                #         mlp_weight.append(value.cpu().numpy())\n",
    "                # mlp_params = os.path.join(model_save_path, model_name + '_' + str((i+1)/100)+'_params.mat')\n",
    "                # io.savemat(mlp_params, {'fc1_weight': mlp_weight[0], 'fc1_bias': mlp_weight[1],'fc2_weight': mlp_weight[2], 'fc2_bias': mlp_weight[3]})\n",
    "                \n",
    "            model.train() \n",
    "\n",
    "    print(f'Trials [{i+1}], Loss: {loss.item():.4f}')  \n",
    "    model.to('cpu')\n",
    "\n",
    "    io.savemat(os.path.join(model_save_path, model_name +'_result.mat'),\n",
    "        {'sub_train_loss':sub_train_loss, 'prediction':prediction_mat, 'hidden_layer':hidden_layer_mat, 'hidden_layer_relu':hidden_layer_relu_mat, 'output_layer':output_layer_mat, \n",
    "            'amplitude': amplitude_mat, 'phase': phase_mat, 'amplitude_training':amplitude_training, 'phase_training': phase_training, 'vtc_input':vtc_input_mat })  \n",
    "    torch.save(model, os.path.join(model_save_path, model_name + '_model.pth'))\n",
    "\n",
    "    return model, sub_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d217bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for A100\n",
    "data_path = r'/Re_analysis_data_path/d010_Gaussian_MLPtraing_set.mat'\n",
    "sub_data = io.loadmat(data_path)['sub_data']\n",
    "\n",
    "X_test = io.loadmat(data_path)['test_trial']\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e3a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials [4000], Loss: 2.5142\n",
      "Trials [4000], Loss: 33.3029\n",
      "Trials [4000], Loss: 24.1694\n",
      "Trials [4000], Loss: 3.0948\n",
      "Trials [4000], Loss: 43.0265\n",
      "Trials [4000], Loss: 1.2950\n",
      "Trials [4000], Loss: 29.9846\n",
      "Trials [4000], Loss: 19.7853\n",
      "Trials [4000], Loss: 1.5991\n",
      "Trials [4000], Loss: 19.8555\n",
      "Trials [4000], Loss: 19.0216\n",
      "Trials [4000], Loss: 1.7291\n",
      "Trials [4000], Loss: 17.9052\n",
      "Trials [4000], Loss: 5.0041\n",
      "Trials [4000], Loss: 26.7315\n",
      "Trials [4000], Loss: 3.6840\n",
      "Trials [4000], Loss: 27.9342\n",
      "Trials [4000], Loss: 3.7098\n",
      "Trials [4000], Loss: 2.9940\n",
      "Trials [4000], Loss: 24.2873\n",
      "Trials [4000], Loss: 27.5901\n",
      "Trials [4000], Loss: 2.1368\n",
      "Trials [4000], Loss: 2.2156\n",
      "Trials [4000], Loss: 21.0818\n",
      "Trials [4000], Loss: 4.4068\n",
      "Trials [4000], Loss: 2.0076\n",
      "Trials [4000], Loss: 14.4292\n",
      "Trials [4000], Loss: 14.5304\n",
      "Trials [4000], Loss: 1.4757\n",
      "Trials [4000], Loss: 2.3089\n",
      "Trials [4000], Loss: 7.6529\n",
      "Trials [4000], Loss: 13.1612\n",
      "Trials [4000], Loss: 1.0458\n",
      "Trials [4000], Loss: 2.7516\n",
      "Trials [4000], Loss: 18.3765\n",
      "Trials [4000], Loss: 25.1275\n",
      "Trials [4000], Loss: 0.5549\n",
      "Trials [4000], Loss: 16.6552\n",
      "Trials [4000], Loss: 21.3827\n",
      "Trials [4000], Loss: 0.8914\n",
      "Trials [4000], Loss: 17.8665\n",
      "Trials [4000], Loss: 0.7600\n",
      "Trials [4000], Loss: 16.7007\n",
      "Trials [4000], Loss: 22.8968\n",
      "Trials [4000], Loss: 0.6344\n",
      "Trials [4000], Loss: 7.4759\n",
      "Trials [4000], Loss: 9.2375\n",
      "Trials [4000], Loss: 1.0587\n",
      "Trials [4000], Loss: 21.4160\n",
      "Trials [4000], Loss: 0.7978\n",
      "Trials [4000], Loss: 25.6352\n",
      "Trials [4000], Loss: 1.4675\n",
      "Trials [4000], Loss: 19.7987\n",
      "Trials [4000], Loss: 0.7436\n",
      "Trials [4000], Loss: 0.9666\n",
      "Trials [4000], Loss: 18.3184\n",
      "Trials [4000], Loss: 31.8962\n",
      "Trials [4000], Loss: 0.7414\n",
      "Trials [4000], Loss: 1.0450\n",
      "Trials [4000], Loss: 26.4606\n",
      "Trials [4000], Loss: 1.0500\n",
      "Trials [4000], Loss: 0.7540\n",
      "Trials [4000], Loss: 11.1081\n",
      "Trials [4000], Loss: 17.4372\n",
      "Trials [4000], Loss: 0.6224\n",
      "Trials [4000], Loss: 0.7857\n",
      "Trials [4000], Loss: 13.1468\n",
      "Trials [4000], Loss: 6.3113\n",
      "Trials [4000], Loss: 1.0743\n",
      "Trials [4000], Loss: 0.9220\n",
      "Trials [4000], Loss: 23.9525\n",
      "Trials [4000], Loss: 17.1820\n"
     ]
    }
   ],
   "source": [
    "# Partial parameter testing\n",
    "model_save_path = r'/your_save_path/Gaussian_sigma_test'\n",
    "\n",
    "# Hyperparameters  \n",
    "cc_angle = np.radians(np.arange(15, 360, step=30))\n",
    "a = np.cos(cc_angle)\n",
    "a = a[:, np.newaxis]\n",
    "b = np.sin(cc_angle)\n",
    "b = b[:, np.newaxis]\n",
    "cc_coord = torch.tensor(np.concatenate((a,b),axis=1))\n",
    "\n",
    "batch_n = 24\n",
    "learning_rate = 0.001 \n",
    "weight_std_list = [0.01, 0.1, 0.5, 1, 2, 3]\n",
    "phaseR_list = [1, 2, 3, 6, 9, 18, 36, 72, 180] \n",
    "amplitudeR_list = [1, 0.8, 0.6, 0.4, 0.2, 0]  \n",
    "modulation_list = ['none', 'phase', 'amplitude', 'both']\n",
    "\n",
    "# amplitudeR = 0.8\n",
    "# phaseR = 3\n",
    "# wi = 0.01\n",
    "\n",
    "for amplitudeR in amplitudeR_list:\n",
    "    for phaseR in phaseR_list:\n",
    "        for wi in weight_std_list:\n",
    "            for neuron_modulation in modulation_list:\n",
    "                for sub_i in range(36):\n",
    "                    x_train = sub_data[sub_i,0].astype(np.float32)\n",
    "                    y_train = sub_data[sub_i,2].astype(np.float32)\n",
    "\n",
    "                    torch_data = GetLoader(x_train, y_train)\n",
    "                    train_datas = DataLoader(torch_data, batch_size=batch_n, shuffle=True, drop_last=False, num_workers=0)\n",
    "\n",
    "                    # Initialize model, criterion, and optimizer  \n",
    "                    model = SimpleNN(20, wi, neuron_modulation, phaseR, amplitudeR) \n",
    "                    criterion = nn.MSELoss()\n",
    "                    # optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "                    optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "                    #  \n",
    "                    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "                    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.8)\n",
    "                    \n",
    "                    # Train the model using single-sample SGD  \n",
    "                    model_name = 'mGaussianMLP_WI' + str(wi) + '_' + neuron_modulation + '_'+ str(phaseR) + '_' + str(amplitudeR)+ '_Sub'+(\"{:02d}\".format(sub_i+1))\n",
    "                    sub_behavior_model, sub_train_loss = train_model_single_sample(model, criterion, optimizer, scheduler, train_datas, X_test, model_save_path, model_name)  \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
